{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c008692-2218-4a1a-8f65-6a9c91a8ae7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax \n",
    "from jax import numpy as jnp\n",
    "import chex\n",
    "import gymnax\n",
    "from gymnax.environments.environment import EnvState, Environment, EnvParams\n",
    "\n",
    "from typing import Tuple, Any, Callable\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5d7da67-60ff-42d2-80a9-37d6cde9ee34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import (\n",
    "    DQNTrainingArgs, DQNTrainState,\n",
    "    DQN, DQNParameters, DQNAgent,\n",
    "    select_action, compute_loss, update_target,\n",
    "    initialize_agent_state,\n",
    "    compute_loss_double_dqn,\n",
    "    SimpleDQNAgent,\n",
    "    DoubleDQNAgent\n",
    ")\n",
    "from buffer import ReplayBuffer, ReplayBufferStorage, FIFOBuffer\n",
    "from trainer import agent_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bdd1c0bd-f5cc-452d-8799-85453efc63ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds_dqn = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2724fb87-f9f9-45f7-8eae-0c97f5c6a426",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape: (4,)\n",
      "Q-values shape: (4, 2)\n",
      "Random action shape: ()\n",
      "Greedy action shape: ()\n",
      "Random number shape: ()\n",
      "Epsilon shape: ()\n",
      "Final action shape: ()\n",
      "State shape: (4,)\n",
      "Q-values shape: (4, 2)\n",
      "Random action shape: ()\n",
      "Greedy action shape: ()\n",
      "Random number shape: ()\n",
      "Epsilon shape: ()\n",
      "Final action shape: ()\n",
      "State shape: (4,)\n",
      "Q-values shape: (4, 2)\n",
      "Random action shape: ()\n",
      "Greedy action shape: ()\n",
      "Random number shape: ()\n",
      "Epsilon shape: ()\n",
      "Final action shape: ()\n",
      "State shape: (4,)\n",
      "Q-values shape: (4, 2)\n",
      "Random action shape: ()\n",
      "Greedy action shape: ()\n",
      "Random number shape: ()\n",
      "Epsilon shape: ()\n",
      "Final action shape: ()\n",
      "State shape: (4,)\n",
      "Q-values shape: (4, 2)\n",
      "Random action shape: ()\n",
      "Greedy action shape: ()\n",
      "Random number shape: ()\n",
      "Epsilon shape: ()\n",
      "Final action shape: ()\n",
      "State shape: (4,)\n",
      "Q-values shape: (4, 2)\n",
      "Random action shape: ()\n",
      "Greedy action shape: ()\n",
      "Random number shape: ()\n",
      "Epsilon shape: ()\n",
      "Final action shape: ()\n",
      "State shape: (4,)\n",
      "Q-values shape: (4, 2)\n",
      "Random action shape: ()\n",
      "Greedy action shape: ()\n",
      "Random number shape: ()\n",
      "Epsilon shape: ()\n",
      "Final action shape: ()\n",
      "State shape: (4,)\n",
      "Q-values shape: (4, 2)\n",
      "Random action shape: ()\n",
      "Greedy action shape: ()\n",
      "Random number shape: ()\n",
      "Epsilon shape: ()\n",
      "Final action shape: ()\n",
      "State shape: (4,)\n",
      "Q-values shape: (4, 2)\n",
      "Random action shape: ()\n",
      "Greedy action shape: ()\n",
      "Random number shape: ()\n",
      "Epsilon shape: ()\n",
      "Final action shape: ()\n",
      "State shape: (4,)\n",
      "Q-values shape: (4, 2)\n",
      "Random action shape: ()\n",
      "Greedy action shape: ()\n",
      "Random number shape: ()\n",
      "Epsilon shape: ()\n",
      "Final action shape: ()\n",
      "State shape: (4,)\n",
      "Q-values shape: (4, 2)\n",
      "Random action shape: ()\n",
      "Greedy action shape: ()\n",
      "Random number shape: ()\n",
      "Epsilon shape: ()\n",
      "Final action shape: ()\n",
      "State shape: (4,)\n",
      "Q-values shape: (4, 2)\n",
      "Random action shape: ()\n",
      "Greedy action shape: ()\n",
      "Random number shape: ()\n",
      "Epsilon shape: ()\n",
      "Final action shape: ()\n",
      "State shape: (4,)\n",
      "Q-values shape: (4, 2)\n",
      "Random action shape: ()\n",
      "Greedy action shape: ()\n",
      "Random number shape: ()\n",
      "Epsilon shape: ()\n",
      "Final action shape: ()\n",
      "State shape: (4,)\n",
      "Q-values shape: (4, 2)\n",
      "Random action shape: ()\n",
      "Greedy action shape: ()\n",
      "Random number shape: ()\n",
      "Epsilon shape: ()\n",
      "Final action shape: ()\n",
      "State shape: (4,)\n",
      "Q-values shape: (4, 2)\n",
      "Random action shape: ()\n",
      "Greedy action shape: ()\n",
      "Random number shape: ()\n",
      "Epsilon shape: ()\n",
      "Final action shape: ()\n",
      "State shape: (4,)\n",
      "Q-values shape: (4, 2)\n",
      "Random action shape: ()\n",
      "Greedy action shape: ()\n",
      "Random number shape: ()\n",
      "Epsilon shape: ()\n",
      "Final action shape: ()\n",
      "State shape: (4,)\n",
      "Q-values shape: (4, 2)\n",
      "Random action shape: ()\n",
      "Greedy action shape: ()\n",
      "Random number shape: ()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m environment_step \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;241m0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mjnp\u001b[38;5;241m.\u001b[39mint32)\n\u001b[0;32m     20\u001b[0m agent_iter \u001b[38;5;241m=\u001b[39m partial(agent_iteration, args, SimpleDQNAgent, FIFOBuffer, env_reset, env_step)\n\u001b[1;32m---> 21\u001b[0m agent_iter \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mjit(agent_iter, donate_argnums\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m,))\u001b[38;5;241m.\u001b[39mlower(\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# states\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     rng, agent_state, buffer_state, env_state,\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m# inputs\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     obs, environment_step\n\u001b[0;32m     26\u001b[0m )\u001b[38;5;241m.\u001b[39mcompile()\n\u001b[0;32m     27\u001b[0m losses \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     28\u001b[0m steps \u001b[38;5;241m=\u001b[39m []\n",
      "    \u001b[1;31m[... skipping hidden 12 frame]\u001b[0m\n",
      "File \u001b[1;32mf:\\course-work MS\\Reinforcement Learning\\DQN\\trainer.py:158\u001b[0m, in \u001b[0;36magent_iteration\u001b[1;34m(args, agent, buffer, env_reset, env_step, rng, agent_state, buffer_state, env_state, last_obs, environment_step)\u001b[0m\n\u001b[0;32m    156\u001b[0m (rng, agent_state, buffer_state, env_state, last_obs, environment_step) \u001b[38;5;241m=\u001b[39m scan_final_state\n\u001b[0;32m    157\u001b[0m \u001b[38;5;66;03m# run several evaluation episodes and compute returns\u001b[39;00m\n\u001b[1;32m--> 158\u001b[0m rng, eval_returns \u001b[38;5;241m=\u001b[39m eval_agent(\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;66;03m# configuration\u001b[39;00m\n\u001b[0;32m    160\u001b[0m     args, agent, buffer, env_reset, env_step,\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;66;03m# states\u001b[39;00m\n\u001b[0;32m    162\u001b[0m     rng, agent_state\n\u001b[0;32m    163\u001b[0m )\n\u001b[0;32m    164\u001b[0m \u001b[38;5;66;03m# set the target network parameters to current parameters\u001b[39;00m\n\u001b[0;32m    165\u001b[0m agent_state \u001b[38;5;241m=\u001b[39m update_target(agent_state)\n",
      "File \u001b[1;32mf:\\course-work MS\\Reinforcement Learning\\DQN\\trainer.py:109\u001b[0m, in \u001b[0;36meval_agent\u001b[1;34m(args, agent, buffer, env_reset, env_step, rng, agent_state)\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    106\u001b[0m         rng, obs, env_state, episodes, returns, last_episode_return\n\u001b[0;32m    107\u001b[0m     ), t\n\u001b[0;32m    108\u001b[0m total_steps \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39meval_env_steps \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m args\u001b[38;5;241m.\u001b[39meval_environments\n\u001b[1;32m--> 109\u001b[0m scan_state, _ \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mlax\u001b[38;5;241m.\u001b[39mscan(\n\u001b[0;32m    110\u001b[0m     one_step,\n\u001b[0;32m    111\u001b[0m     (rng, obs, env_state, episodes, returns, last_episode_return),\n\u001b[0;32m    112\u001b[0m     jnp\u001b[38;5;241m.\u001b[39marange(total_steps)\n\u001b[0;32m    113\u001b[0m )\n\u001b[0;32m    114\u001b[0m rng, obs, env_state, episodes, returns, last_episode_return \u001b[38;5;241m=\u001b[39m scan_state\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rng, (returns \u001b[38;5;241m/\u001b[39m episodes)\u001b[38;5;241m.\u001b[39mmean()\n",
      "    \u001b[1;31m[... skipping hidden 9 frame]\u001b[0m\n",
      "File \u001b[1;32mf:\\course-work MS\\Reinforcement Learning\\DQN\\trainer.py:93\u001b[0m, in \u001b[0;36meval_agent.<locals>.one_step\u001b[1;34m(scan_state, t)\u001b[0m\n\u001b[0;32m     91\u001b[0m step_rng \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(step_rng, args\u001b[38;5;241m.\u001b[39meval_environments)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# if we set epsilon to zero, eval becomes greedy, so we set the last arg to 0.0\u001b[39;00m\n\u001b[1;32m---> 93\u001b[0m action \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mvmap(agent\u001b[38;5;241m.\u001b[39mselect_action, in_axes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))(\n\u001b[0;32m     94\u001b[0m     agent\u001b[38;5;241m.\u001b[39mdqn, action_rng, agent_state\u001b[38;5;241m.\u001b[39mparams, obs, \u001b[38;5;241m0.0\u001b[39m) \n\u001b[0;32m     95\u001b[0m obs, env_state, reward, done, info \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mvmap(env_step)(step_rng, env_state, action)\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# done is zero for all steps except one in each episode \u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# so if we sum all dones in the trajectory, we count episodes\u001b[39;00m\n",
      "    \u001b[1;31m[... skipping hidden 3 frame]\u001b[0m\n",
      "File \u001b[1;32mf:\\course-work MS\\Reinforcement Learning\\DQN\\model.py:125\u001b[0m, in \u001b[0;36mselect_action\u001b[1;34m(dqn, rng, params, state, epsilon)\u001b[0m\n\u001b[0;32m    123\u001b[0m random_number \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(eps_key)\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRandom number shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, random_number\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m--> 125\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpsilon shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, epsilon\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m    127\u001b[0m \u001b[38;5;66;03m# Convert to arrays with same shape if needed\u001b[39;00m\n\u001b[0;32m    128\u001b[0m random_action \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39marray(random_action)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "for SEED in tqdm(range(40, 51)):\n",
    "    args = DQNTrainingArgs()\n",
    "    rng = jax.random.key(SEED)\n",
    "    rng, agent_init_rng = jax.random.split(rng, 2)\n",
    "    # create the agent and its optimization state\n",
    "    agent_state = SimpleDQNAgent.initialize_agent_state(SimpleDQNAgent.dqn, agent_init_rng, args)\n",
    "    # create the environment\n",
    "    env, env_params = gymnax.make('CartPole-v1')\n",
    "    def env_reset(rng: chex.PRNGKey) -> Tuple[chex.Array, EnvState]:\n",
    "        return env.reset(rng, env_params)\n",
    "    def env_step(rng: chex.PRNGKey, env_state: EnvState, action: chex.Array) -> Tuple[chex.Array, EnvState]:\n",
    "        return env.step(rng, env_state, action, env_params)\n",
    "    state_shape = env.observation_space(env_params).shape\n",
    "    n_actions = env.action_space().n\n",
    "    buffer_state = FIFOBuffer.init_buffer(buffer_size=args.fifo_buffer_size, state_shape=state_shape)\n",
    "    rng, reset_rng = jax.random.split(rng, 2)\n",
    "    obs, env_state = env_reset(reset_rng)\n",
    "    environment_step = jnp.array(0, dtype=jnp.int32)\n",
    "    \n",
    "    agent_iter = partial(agent_iteration, args, SimpleDQNAgent, FIFOBuffer, env_reset, env_step)\n",
    "    agent_iter = jax.jit(agent_iter, donate_argnums=(2,)).lower(\n",
    "        # states\n",
    "        rng, agent_state, buffer_state, env_state,\n",
    "        # inputs\n",
    "        obs, environment_step\n",
    "    ).compile()\n",
    "    losses = []\n",
    "    steps = []\n",
    "    returns = []\n",
    "    while environment_step < args.sample_budget:\n",
    "        rng, agent_state, buffer_state, env_state, obs, environment_step, dqn_losses, eval_returns = agent_iter(\n",
    "\n",
    "            rng, agent_state, buffer_state, env_state,\n",
    "            # inputs\n",
    "            obs, environment_step\n",
    "        )\n",
    "        # print(environment_step)\n",
    "        steps.append(environment_step)\n",
    "        returns.append(eval_returns)\n",
    "        losses.append(dqn_losses)\n",
    "        # return returns, losses\n",
    "    seeds_dqn[SEED] = (steps, returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afdbff8-5988-4069-8029-8c67401dbfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,3))\n",
    "plt.tight_layout()\n",
    "mean_return = np.stack([seeds_dqn[k][1] for k in seeds_dqn.keys()]).mean(0)\n",
    "std_return = np.stack([seeds_dqn[k][1] for k in seeds_dqn.keys()]).std(0)\n",
    "plt.plot(steps, mean_return, label='dqn')\n",
    "plt.fill_between(steps, mean_return - std_return, mean_return + std_return, alpha=0.3)\n",
    "plt.ylim(0, 500)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.savefig('dqn.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ba85436-6ce8-4510-af63-677e3159dbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds_ddqn = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0694ce94-c049-4888-aff4-9e2e8ad7d8b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tqdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m SEED \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m40\u001b[39m, \u001b[38;5;241m51\u001b[39m)):\n\u001b[0;32m      2\u001b[0m     args \u001b[38;5;241m=\u001b[39m DQNTrainingArgs()\n\u001b[0;32m      3\u001b[0m     rng \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mPRNGKey(SEED)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tqdm' is not defined"
     ]
    }
   ],
   "source": [
    "for SEED in tqdm(range(40, 51)):\n",
    "    args = DQNTrainingArgs()\n",
    "    rng = jax.random.PRNGKey(SEED)\n",
    "    rng, agent_init_rng = jax.random.split(rng, 2)\n",
    "    # create the agent and its optimization state\n",
    "    agent_state = DoubleDQNAgent.initialize_agent_state(DoubleDQNAgent.dqn, agent_init_rng, args)\n",
    "    # create the environment\n",
    "    env, env_params = gymnax.make('CartPole-v1')\n",
    "    def env_reset(rng: chex.PRNGKey) -> Tuple[chex.Array, EnvState]:\n",
    "        return env.reset(rng, env_params)\n",
    "    def env_step(rng: chex.PRNGKey, env_state: EnvState, action: chex.Array) -> Tuple[chex.Array, EnvState]:\n",
    "        return env.step(rng, env_state, action, env_params)\n",
    "    state_shape = env.observation_space(env_params).shape\n",
    "    n_actions = env.action_space().n\n",
    "    # create replay buffer storage\n",
    "    buffer_state = FIFOBuffer.init_buffer(buffer_size=args.fifo_buffer_size, state_shape=state_shape)\n",
    "    # reset the environment to start working with it\n",
    "    rng, reset_rng = jax.random.split(rng, 2)\n",
    "    obs, env_state = env_reset(reset_rng)\n",
    "    environment_step = jnp.array(0, dtype=jnp.int32)\n",
    "\n",
    "    # now we define the main function to update the agent and compile it\n",
    "    agent_iter = partial(agent_iteration, args, DoubleDQNAgent, FIFOBuffer, env_reset, env_step)\n",
    "    # donate_argnums=(2,) tells jax to optimize all operations with replay buffer\n",
    "    # and do them in-place\n",
    "    # the cost of this to that we need to recompile it every time we reinitialize the agent\n",
    "    # (because some functions get recreated). we could avoid this at the cost of a bit \n",
    "    # more complicated implementation.\n",
    "    agent_iter = jax.jit(agent_iter, donate_argnums=(2,)).lower(\n",
    "        # states\n",
    "        rng, agent_state, buffer_state, env_state,\n",
    "        # inputs\n",
    "        obs, environment_step\n",
    "    ).compile()\n",
    "    losses = []\n",
    "    steps = []\n",
    "    returns = []\n",
    "    while environment_step < args.sample_budget:\n",
    "        rng, agent_state, buffer_state, env_state, obs, environment_step, dqn_losses, eval_returns = agent_iter(\n",
    "            # states\n",
    "            rng, agent_state, buffer_state, env_state,\n",
    "            # inputs\n",
    "            obs, environment_step\n",
    "        )\n",
    "        # print(environment_step)\n",
    "        steps.append(environment_step)\n",
    "        returns.append(eval_returns)\n",
    "        losses.append(dqn_losses)\n",
    "        # return returns, losses\n",
    "    seeds_ddqn[SEED] = (steps, returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23603560-5a12-40ff-8992-17f49325fcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,3))\n",
    "plt.tight_layout()\n",
    "mean_return = np.stack([seeds_dqn[k][1] for k in seeds_dqn.keys()]).mean(0)\n",
    "std_return = np.stack([seeds_dqn[k][1] for k in seeds_dqn.keys()]).std(0)\n",
    "plt.plot(steps, mean_return, label='dqn')\n",
    "plt.fill_between(steps, mean_return - std_return, mean_return + std_return, alpha=0.3)\n",
    "\n",
    "mean_return = np.stack([seeds_ddqn[k][1] for k in seeds_ddqn.keys()]).mean(0)\n",
    "std_return = np.stack([seeds_ddqn[k][1] for k in seeds_ddqn.keys()]).std(0)\n",
    "plt.plot(steps, mean_return, label='double-dqn')\n",
    "plt.fill_between(steps, mean_return - std_return, mean_return + std_return, alpha=0.3)\n",
    "\n",
    "plt.ylim(0, 500)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.savefig('double-dqn.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7287f431-b846-4b8a-b3c3-b894e9adcb88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
